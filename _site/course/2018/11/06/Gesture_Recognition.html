<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Gesture Recognition</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <head>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber:     "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"     type="text/javascript"></script>
</head>

  </head>
    <body>
      <header class="site-header">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">MvO</a>
  
    
      <nav class="site-nav">
        <!--<input type="checkbox" id="nav-trigger" class="nav-trigger" />-->
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
            <a class="page-link" href="/about.html">About</a>
            
          
            
            
            <a class="page-link" href="/">Projects</a>
            
          
            
            
          
            
            
            <a class="page-link" href="/thesis.html">Thesis</a>
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>

      <div class="page-content">
        <div class="wrapper">
      <h1>Gesture Recognition</h1>
<p>
  06 Nov 2018
</p>

<p>This project was done in  collabaration with @vhrgitai and @faaip, for the repo see <a href="https://github.com/faaip/OpenPose-Gesture-Recognition">github</a>.</p>

<h1 id="openpose-gesture-recognition">OpenPose Gesture Recognition</h1>

<p>In this project we made a classifier, to classify the videos from the <a href="https://20bn.com/datasets/jester">20bn jester dataset</a>
This dataset contains videos with gestures made by people from all over the world, by using their webcam.</p>

<p>To preprocess the videos (seen as a series of images), we used <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/">OpenPose</a> posture recognition from Carnegie Mellon University’s Perceptual Computing Lab. This way each video was converted to a sequence of body-point registrations.</p>

<p>A video below shows an example of the extracted body points projected back on the original video, as you can see not every image get’s succesfully parsed.</p>

<p><img src="/assets/images/gesture_recog/animation.gif" alt="Example video" /> <img src="/assets/images/gesture_recog/processed.gif" alt="parsed video" /></p>

<h1 id="training-the-classifier">Training the Classifier</h1>

<p>The architecture of the RNN is based on <a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition">guillaume-chevalier</a>. The architecture consists of an fully connected layer followed by two LSTM cell, then the eventual classification is made by two fully connected layers again.</p>

<p>The cost function is the cross entropy function for the prediction together with a regularization term to keep the model from overfitting, this function was optimized using the AdamOptimizer.</p>

<p>For training of the classifier we took our dataset and randomly seperate it in a training set of $14000$ and a test set of $3054$.</p>

<h1 id="results">Results</h1>

<p>The classifier ended up with an accuracy off roughly $50\%$. We want to emphasize that we used only $10\%$ of the total dataset, this was because of computational limitations. The extaction of the datapoints from the videos took longer that expected. Our hope is that the accuracy will increase with the size of the dataset. Training the classifier on more data will hopefully make it able to distringuish more between similar gestures. The confusion matrix shows what kind of mistakes the classifier was making.</p>

<p><img src="/assets/images/gesture_recog/confusion_matrix_all.png" alt="Confussion Matrix" heigth="700px" width="700px" /></p>


        </div>
      </div>

    </body>
</html>
